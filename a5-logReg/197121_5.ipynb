{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Assignment 5\n",
    "##### Name: Venkata Sai Manoj Boganadham\n",
    "##### Roll no: 197121\n",
    "##### Section: A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear Regression using Sklearn library\n",
    "\n",
    "Here we are performing Linear Regression on the boston housing dataset that we have already done from scratch.\n",
    "\n",
    "First let us import the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First we need to load the dataset\n",
    "For that, we will use the dataset loading API built into sklearn. \n",
    "Then, we need to get the dependent and independent variables from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n"
     ]
    }
   ],
   "source": [
    "# Import boston data from sklearn\n",
    "\n",
    "# This function is deprecated by sklearn, \n",
    "# raw_data = load_boston()\n",
    "\n",
    "# This is the suggested way to remote import boston dataset\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "# Boston dataframe imported\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "# Dependent variables and independent values are separated\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "Y = raw_df.values[1::2, 2]\n",
    "\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting the dataset into training and testing parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13) (102, 13) (404,) (102,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dependent and independent variables into training and testing datasets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=25)\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, we are ready to build the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler(with_mean=False)),\n",
       "                ('linearregression', LinearRegression())])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the linear regression model using sklearn\n",
    "\n",
    "# We are enabling standardization of data here\n",
    "# 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
    "# model = lr(normalize = True)\n",
    "\n",
    "# If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
    "model = make_pipeline(StandardScaler(with_mean = False), lr())\n",
    "\n",
    "# Fitting the model, i.e. finding the parameters using gradient descent\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we make the predictions out of the model built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102,)\n",
      "Root Mean Squared Error of the model is:  4.8360470651852205\n"
     ]
    }
   ],
   "source": [
    "# Making predictions using sklearn\n",
    "Y_pred = model.predict(X_test)\n",
    "print(Y_pred.shape)\n",
    "\n",
    "# Getting the Root Mean Squared Error\n",
    "# When 'squared' is set to False, this method returns the RMSE\n",
    "rmse = mse(Y_test, Y_pred, squared = False)\n",
    "print(\"Root Mean Squared Error of the model is: \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is to be observed that we got the approimately same error when we applied Linear Regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logisitic Regression on Heart Disease data set\n",
    "\n",
    "Data set link: https://www.kaggle.com/ronitf/heart-disease-uci?select=heart.csv\n",
    "\n",
    "First we need to import required libraries and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "raw_data = pd.read_csv('heart.csv')\n",
    "print(raw_data.shape)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: (242, 14)\n",
      "Test data: (61, 14)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the dataframe\n",
    "train_data = raw_data.sample(frac=0.8, random_state=42)\n",
    "test_data = raw_data.drop(train_data.index)\n",
    "\n",
    "print(\"Train data:\", train_data.shape)\n",
    "print(\"Test data:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the raw data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 13) (61, 13) (242,) (61,)\n"
     ]
    }
   ],
   "source": [
    "# Creating the numpy arrays X_train, X_test, Y_train, Y_test\n",
    "X_train = train_data[train_data.columns[:-1]]\n",
    "Y_train = train_data[train_data.columns[-1]]\n",
    "X_test = test_data[test_data.columns[:-1]]\n",
    "Y_test = test_data[test_data.columns[-1]]\n",
    "\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalising the X data of training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalised training data\n",
      "          age       sex        cp  trestbps      chol       fbs   restecg  \\\n",
      "179  0.288049  0.675233 -0.960180  1.022729  0.511790 -0.423960 -1.000167   \n",
      "228  0.503863  0.675233  1.956517  2.145690  0.742072 -0.423960 -1.000167   \n",
      "111  0.288049  0.675233  0.984285  1.022729 -2.366732  2.348967  0.905663   \n",
      "246  0.180142 -1.474851 -0.960180  0.124361  3.064080 -0.423960 -1.000167   \n",
      "60   1.798745 -1.474851  0.984285 -1.223192  0.300699  2.348967 -1.000167   \n",
      "\n",
      "      thalach     exang   oldpeak     slope        ca      thal  \n",
      "179 -1.703583  1.489116 -0.380228 -0.641393  0.264617 -2.116461  \n",
      "228  0.380814 -0.668765 -0.719142 -0.641393 -0.705644  1.145855  \n",
      "111  1.001698 -0.668765 -0.719142  0.958784  0.264617  1.145855  \n",
      "246 -0.018326  1.489116  0.721243 -0.641393  1.234878  1.145855  \n",
      "60  -0.905303 -0.668765 -0.888600  0.958784  0.264617 -0.485303  \n",
      "         age       sex        cp  trestbps      chol       fbs   restecg  \\\n",
      "1  -2.089834  0.692682  1.061296 -0.059467  0.322834 -0.385317  0.858624   \n",
      "3   0.178066  0.692682  0.109251 -0.664049  0.038607 -0.385317  0.858624   \n",
      "13  1.132972  0.692682  2.013341 -1.268632 -0.468942 -0.385317 -1.011949   \n",
      "20  0.536156  0.692682 -0.842794  0.242824 -0.001997 -0.385317  0.858624   \n",
      "21 -1.254292  0.692682  1.061296 -0.059467 -0.022299 -0.385317  0.858624   \n",
      "\n",
      "     thalach     exang   oldpeak     slope        ca      thal  \n",
      "1   1.667581 -0.798758  2.290034 -2.380570 -0.738917 -0.618088  \n",
      "3   1.296030 -0.798758 -0.186451  1.036248 -0.738917 -0.618088  \n",
      "13 -0.107608  1.231419  0.730766 -0.672161 -0.738917 -0.618088  \n",
      "20  0.594211 -0.798758 -0.461616 -0.672161 -0.738917  1.021188  \n",
      "21  1.337314  1.231419 -0.553337  1.036248 -0.738917 -0.618088  \n"
     ]
    }
   ],
   "source": [
    "# Function to normalise the dataframe\n",
    "def norm_dataframe(X):\n",
    "    X = (X - X.mean())/X.std()\n",
    "    return X\n",
    "\n",
    "\n",
    "# Normalising the training and testing data\n",
    "X_train = norm_dataframe(X_train)\n",
    "X_test = norm_dataframe(X_test)\n",
    "\n",
    "print(\"Normalised training data\")\n",
    "print(X_train.head())\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the numpy arrays out of the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the matrices after splitting from the raw data\n",
      "(242, 13)\n",
      "(242, 1)\n",
      "(61, 13)\n",
      "(61, 1)\n",
      "Shapes of the matrices after concatenating ones\n",
      "(242, 14)\n",
      "(242, 1)\n",
      "(61, 14)\n",
      "(61, 1)\n"
     ]
    }
   ],
   "source": [
    "# Generating the numpy arrays\n",
    "X_train = X_train.values\n",
    "Y_train = Y_train.values\n",
    "X_test = X_test.values\n",
    "Y_test = Y_test.values\n",
    "\n",
    "# Reshaping train data into 2d matrices\n",
    "Y_train = Y_train.reshape(len(Y_train), 1)\n",
    "Y_test = Y_test.reshape(len(Y_test), 1)\n",
    "\n",
    "print(\"Shapes of the matrices after splitting from the raw data\")\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "# Concatenating the training data with a column of ones\n",
    "X_train = np.concatenate((np.ones(shape = Y_train.shape, dtype = np.float64), X_train), axis = 1)\n",
    "X_test = np.concatenate((np.ones(shape = Y_test.shape, dtype = np.float64), X_test), axis = 1)\n",
    "\n",
    "print(\"Shapes of the matrices after concatenating ones\")\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is split and scaled, now we can proceed to build our model\n",
    "### Building the logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis of logistic regression\n",
    "def h(X_train, theta):\n",
    "    hyp = np.matmul(X_train, theta)\n",
    "    g = 1/(1 + np.exp(-hyp))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(X_train, Y_train, theta):\n",
    "    m = len(Y_train)\n",
    "    g = h(X_train, theta)\n",
    "    Y_t = np.transpose(Y_train)\n",
    "    J = -(1/m) * (Y_t @ np.log(g) + (1-Y_t) @ np.log(1-g))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch  0  cost is:  [[0.45167065]]\n",
      "At epoch  1  cost is:  [[0.4112882]]\n",
      "At epoch  2  cost is:  [[0.39325149]]\n",
      "At epoch  3  cost is:  [[0.3830585]]\n",
      "At epoch  4  cost is:  [[0.37659885]]\n",
      "At epoch  5  cost is:  [[0.37221042]]\n",
      "At epoch  6  cost is:  [[0.36908453]]\n",
      "At epoch  7  cost is:  [[0.3667795]]\n",
      "At epoch  8  cost is:  [[0.36503403]]\n",
      "At epoch  9  cost is:  [[0.36368422]]\n",
      "At epoch  10  cost is:  [[0.36262242]]\n",
      "At epoch  11  cost is:  [[0.36177536]]\n",
      "At epoch  12  cost is:  [[0.36109161]]\n",
      "At epoch  13  cost is:  [[0.36053418]]\n",
      "At epoch  14  cost is:  [[0.36007586]]\n",
      "At epoch  15  cost is:  [[0.35969627]]\n",
      "At epoch  16  cost is:  [[0.35937989]]\n",
      "At epoch  17  cost is:  [[0.35911475]]\n",
      "At epoch  18  cost is:  [[0.35889148]]\n",
      "At epoch  19  cost is:  [[0.35870266]]\n",
      "At epoch  20  cost is:  [[0.35854238]]\n",
      "At epoch  21  cost is:  [[0.35840586]]\n",
      "At epoch  22  cost is:  [[0.35828925]]\n",
      "At epoch  23  cost is:  [[0.35818936]]\n",
      "At epoch  24  cost is:  [[0.3581036]]\n",
      "At epoch  25  cost is:  [[0.35802981]]\n",
      "At epoch  26  cost is:  [[0.35796619]]\n",
      "At epoch  27  cost is:  [[0.35791123]]\n",
      "At epoch  28  cost is:  [[0.35786369]]\n",
      "At epoch  29  cost is:  [[0.3578225]]\n",
      "At epoch  30  cost is:  [[0.35778676]]\n",
      "At epoch  31  cost is:  [[0.35775571]]\n",
      "At epoch  32  cost is:  [[0.3577287]]\n",
      "At epoch  33  cost is:  [[0.35770519]]\n",
      "At epoch  34  cost is:  [[0.35768469]]\n",
      "At epoch  35  cost is:  [[0.35766681]]\n",
      "At epoch  36  cost is:  [[0.35765119]]\n",
      "At epoch  37  cost is:  [[0.35763754]]\n",
      "At epoch  38  cost is:  [[0.3576256]]\n",
      "At epoch  39  cost is:  [[0.35761516]]\n",
      "At epoch  40  cost is:  [[0.357606]]\n",
      "At epoch  41  cost is:  [[0.35759798]]\n",
      "At epoch  42  cost is:  [[0.35759094]]\n",
      "At epoch  43  cost is:  [[0.35758477]]\n",
      "At epoch  44  cost is:  [[0.35757935]]\n",
      "At epoch  45  cost is:  [[0.35757458]]\n",
      "At epoch  46  cost is:  [[0.35757039]]\n",
      "At epoch  47  cost is:  [[0.35756671]]\n",
      "At epoch  48  cost is:  [[0.35756347]]\n",
      "At epoch  49  cost is:  [[0.35756061]]\n",
      "At epoch  50  cost is:  [[0.3575581]]\n",
      "At epoch  51  cost is:  [[0.35755589]]\n",
      "At epoch  52  cost is:  [[0.35755393]]\n",
      "At epoch  53  cost is:  [[0.35755221]]\n",
      "At epoch  54  cost is:  [[0.35755069]]\n",
      "At epoch  55  cost is:  [[0.35754935]]\n",
      "At epoch  56  cost is:  [[0.35754816]]\n",
      "At epoch  57  cost is:  [[0.35754712]]\n",
      "At epoch  58  cost is:  [[0.35754619]]\n",
      "At epoch  59  cost is:  [[0.35754537]]\n",
      "At epoch  60  cost is:  [[0.35754465]]\n",
      "At epoch  61  cost is:  [[0.35754401]]\n",
      "At epoch  62  cost is:  [[0.35754345]]\n",
      "At epoch  63  cost is:  [[0.35754294]]\n",
      "At epoch  64  cost is:  [[0.3575425]]\n",
      "At epoch  65  cost is:  [[0.35754211]]\n",
      "At epoch  66  cost is:  [[0.35754176]]\n",
      "At epoch  67  cost is:  [[0.35754145]]\n",
      "At epoch  68  cost is:  [[0.35754117]]\n",
      "At epoch  69  cost is:  [[0.35754093]]\n",
      "At epoch  70  cost is:  [[0.35754071]]\n",
      "At epoch  71  cost is:  [[0.35754052]]\n",
      "At epoch  72  cost is:  [[0.35754035]]\n",
      "At epoch  73  cost is:  [[0.3575402]]\n",
      "At epoch  74  cost is:  [[0.35754007]]\n",
      "At epoch  75  cost is:  [[0.35753995]]\n",
      "At epoch  76  cost is:  [[0.35753984]]\n",
      "At epoch  77  cost is:  [[0.35753975]]\n",
      "At epoch  78  cost is:  [[0.35753966]]\n",
      "At epoch  79  cost is:  [[0.35753959]]\n",
      "At epoch  80  cost is:  [[0.35753952]]\n",
      "At epoch  81  cost is:  [[0.35753946]]\n",
      "At epoch  82  cost is:  [[0.35753941]]\n",
      "At epoch  83  cost is:  [[0.35753936]]\n",
      "At epoch  84  cost is:  [[0.35753932]]\n",
      "At epoch  85  cost is:  [[0.35753928]]\n",
      "At epoch  86  cost is:  [[0.35753925]]\n",
      "At epoch  87  cost is:  [[0.35753922]]\n",
      "At epoch  88  cost is:  [[0.35753919]]\n",
      "At epoch  89  cost is:  [[0.35753917]]\n",
      "At epoch  90  cost is:  [[0.35753915]]\n",
      "At epoch  91  cost is:  [[0.35753913]]\n",
      "At epoch  92  cost is:  [[0.35753911]]\n",
      "At epoch  93  cost is:  [[0.35753909]]\n",
      "At epoch  94  cost is:  [[0.35753908]]\n",
      "At epoch  95  cost is:  [[0.35753907]]\n",
      "At epoch  96  cost is:  [[0.35753906]]\n",
      "At epoch  97  cost is:  [[0.35753905]]\n",
      "At epoch  98  cost is:  [[0.35753904]]\n",
      "At epoch  99  cost is:  [[0.35753903]]\n",
      "At epoch  100  cost is:  [[0.35753902]]\n",
      "At epoch  101  cost is:  [[0.35753902]]\n",
      "At epoch  102  cost is:  [[0.35753901]]\n",
      "At epoch  103  cost is:  [[0.35753901]]\n",
      "At epoch  104  cost is:  [[0.357539]]\n",
      "At epoch  105  cost is:  [[0.357539]]\n",
      "At epoch  106  cost is:  [[0.357539]]\n",
      "At epoch  107  cost is:  [[0.35753899]]\n",
      "At epoch  108  cost is:  [[0.35753899]]\n",
      "At epoch  109  cost is:  [[0.35753899]]\n",
      "At epoch  110  cost is:  [[0.35753898]]\n",
      "At epoch  111  cost is:  [[0.35753898]]\n",
      "At epoch  112  cost is:  [[0.35753898]]\n",
      "At epoch  113  cost is:  [[0.35753898]]\n",
      "At epoch  114  cost is:  [[0.35753898]]\n",
      "At epoch  115  cost is:  [[0.35753898]]\n",
      "At epoch  116  cost is:  [[0.35753897]]\n",
      "At epoch  117  cost is:  [[0.35753897]]\n",
      "At epoch  118  cost is:  [[0.35753897]]\n",
      "At epoch  119  cost is:  [[0.35753897]]\n",
      "At epoch  120  cost is:  [[0.35753897]]\n",
      "At epoch  121  cost is:  [[0.35753897]]\n",
      "At epoch  122  cost is:  [[0.35753897]]\n",
      "At epoch  123  cost is:  [[0.35753897]]\n",
      "At epoch  124  cost is:  [[0.35753897]]\n",
      "At epoch  125  cost is:  [[0.35753897]]\n",
      "At epoch  126  cost is:  [[0.35753897]]\n",
      "At epoch  127  cost is:  [[0.35753897]]\n",
      "At epoch  128  cost is:  [[0.35753897]]\n",
      "At epoch  129  cost is:  [[0.35753897]]\n",
      "At epoch  130  cost is:  [[0.35753897]]\n",
      "At epoch  131  cost is:  [[0.35753897]]\n",
      "At epoch  132  cost is:  [[0.35753897]]\n",
      "At epoch  133  cost is:  [[0.35753897]]\n",
      "At epoch  134  cost is:  [[0.35753897]]\n",
      "At epoch  135  cost is:  [[0.35753896]]\n",
      "At epoch  136  cost is:  [[0.35753896]]\n",
      "At epoch  137  cost is:  [[0.35753896]]\n",
      "At epoch  138  cost is:  [[0.35753896]]\n",
      "At epoch  139  cost is:  [[0.35753896]]\n",
      "At epoch  140  cost is:  [[0.35753896]]\n",
      "At epoch  141  cost is:  [[0.35753896]]\n",
      "At epoch  142  cost is:  [[0.35753896]]\n",
      "At epoch  143  cost is:  [[0.35753896]]\n",
      "At epoch  144  cost is:  [[0.35753896]]\n",
      "At epoch  145  cost is:  [[0.35753896]]\n",
      "At epoch  146  cost is:  [[0.35753896]]\n",
      "At epoch  147  cost is:  [[0.35753896]]\n",
      "At epoch  148  cost is:  [[0.35753896]]\n",
      "At epoch  149  cost is:  [[0.35753896]]\n",
      "At epoch  150  cost is:  [[0.35753896]]\n",
      "At epoch  151  cost is:  [[0.35753896]]\n",
      "At epoch  152  cost is:  [[0.35753896]]\n",
      "At epoch  153  cost is:  [[0.35753896]]\n",
      "At epoch  154  cost is:  [[0.35753896]]\n",
      "At epoch  155  cost is:  [[0.35753896]]\n",
      "At epoch  156  cost is:  [[0.35753896]]\n",
      "At epoch  157  cost is:  [[0.35753896]]\n",
      "At epoch  158  cost is:  [[0.35753896]]\n",
      "At epoch  159  cost is:  [[0.35753896]]\n",
      "At epoch  160  cost is:  [[0.35753896]]\n",
      "At epoch  161  cost is:  [[0.35753896]]\n",
      "At epoch  162  cost is:  [[0.35753896]]\n",
      "At epoch  163  cost is:  [[0.35753896]]\n",
      "At epoch  164  cost is:  [[0.35753896]]\n",
      "At epoch  165  cost is:  [[0.35753896]]\n",
      "At epoch  166  cost is:  [[0.35753896]]\n",
      "At epoch  167  cost is:  [[0.35753896]]\n",
      "At epoch  168  cost is:  [[0.35753896]]\n",
      "At epoch  169  cost is:  [[0.35753896]]\n",
      "At epoch  170  cost is:  [[0.35753896]]\n",
      "At epoch  171  cost is:  [[0.35753896]]\n",
      "At epoch  172  cost is:  [[0.35753896]]\n",
      "At epoch  173  cost is:  [[0.35753896]]\n",
      "At epoch  174  cost is:  [[0.35753896]]\n",
      "At epoch  175  cost is:  [[0.35753896]]\n",
      "At epoch  176  cost is:  [[0.35753896]]\n",
      "At epoch  177  cost is:  [[0.35753896]]\n",
      "At epoch  178  cost is:  [[0.35753896]]\n",
      "At epoch  179  cost is:  [[0.35753896]]\n",
      "At epoch  180  cost is:  [[0.35753896]]\n",
      "At epoch  181  cost is:  [[0.35753896]]\n",
      "At epoch  182  cost is:  [[0.35753896]]\n",
      "At epoch  183  cost is:  [[0.35753896]]\n",
      "At epoch  184  cost is:  [[0.35753896]]\n",
      "At epoch  185  cost is:  [[0.35753896]]\n",
      "At epoch  186  cost is:  [[0.35753896]]\n",
      "At epoch  187  cost is:  [[0.35753896]]\n",
      "At epoch  188  cost is:  [[0.35753896]]\n",
      "At epoch  189  cost is:  [[0.35753896]]\n",
      "At epoch  190  cost is:  [[0.35753896]]\n",
      "At epoch  191  cost is:  [[0.35753896]]\n",
      "At epoch  192  cost is:  [[0.35753896]]\n",
      "At epoch  193  cost is:  [[0.35753896]]\n",
      "At epoch  194  cost is:  [[0.35753896]]\n",
      "At epoch  195  cost is:  [[0.35753896]]\n",
      "At epoch  196  cost is:  [[0.35753896]]\n",
      "At epoch  197  cost is:  [[0.35753896]]\n",
      "At epoch  198  cost is:  [[0.35753896]]\n",
      "At epoch  199  cost is:  [[0.35753896]]\n",
      "At epoch  200  cost is:  [[0.35753896]]\n",
      "At epoch  201  cost is:  [[0.35753896]]\n",
      "At epoch  202  cost is:  [[0.35753896]]\n",
      "At epoch  203  cost is:  [[0.35753896]]\n",
      "At epoch  204  cost is:  [[0.35753896]]\n",
      "At epoch  205  cost is:  [[0.35753896]]\n",
      "At epoch  206  cost is:  [[0.35753896]]\n",
      "At epoch  207  cost is:  [[0.35753896]]\n",
      "At epoch  208  cost is:  [[0.35753896]]\n",
      "At epoch  209  cost is:  [[0.35753896]]\n",
      "At epoch  210  cost is:  [[0.35753896]]\n",
      "At epoch  211  cost is:  [[0.35753896]]\n",
      "At epoch  212  cost is:  [[0.35753896]]\n",
      "At epoch  213  cost is:  [[0.35753896]]\n",
      "At epoch  214  cost is:  [[0.35753896]]\n",
      "At epoch  215  cost is:  [[0.35753896]]\n",
      "At epoch  216  cost is:  [[0.35753896]]\n",
      "At epoch  217  cost is:  [[0.35753896]]\n",
      "At epoch  218  cost is:  [[0.35753896]]\n",
      "At epoch  219  cost is:  [[0.35753896]]\n",
      "At epoch  220  cost is:  [[0.35753896]]\n",
      "At epoch  221  cost is:  [[0.35753896]]\n",
      "At epoch  222  cost is:  [[0.35753896]]\n",
      "At epoch  223  cost is:  [[0.35753896]]\n",
      "At epoch  224  cost is:  [[0.35753896]]\n",
      "At epoch  225  cost is:  [[0.35753896]]\n",
      "At epoch  226  cost is:  [[0.35753896]]\n",
      "At epoch  227  cost is:  [[0.35753896]]\n",
      "At epoch  228  cost is:  [[0.35753896]]\n",
      "At epoch  229  cost is:  [[0.35753896]]\n",
      "At epoch  230  cost is:  [[0.35753896]]\n",
      "At epoch  231  cost is:  [[0.35753896]]\n",
      "At epoch  232  cost is:  [[0.35753896]]\n",
      "At epoch  233  cost is:  [[0.35753896]]\n",
      "At epoch  234  cost is:  [[0.35753896]]\n",
      "At epoch  235  cost is:  [[0.35753896]]\n",
      "At epoch  236  cost is:  [[0.35753896]]\n",
      "At epoch  237  cost is:  [[0.35753896]]\n",
      "At epoch  238  cost is:  [[0.35753896]]\n",
      "At epoch  239  cost is:  [[0.35753896]]\n",
      "At epoch  240  cost is:  [[0.35753896]]\n",
      "At epoch  241  cost is:  [[0.35753896]]\n",
      "At epoch  242  cost is:  [[0.35753896]]\n",
      "At epoch  243  cost is:  [[0.35753896]]\n",
      "At epoch  244  cost is:  [[0.35753896]]\n",
      "At epoch  245  cost is:  [[0.35753896]]\n",
      "At epoch  246  cost is:  [[0.35753896]]\n",
      "At epoch  247  cost is:  [[0.35753896]]\n",
      "At epoch  248  cost is:  [[0.35753896]]\n",
      "At epoch  249  cost is:  [[0.35753896]]\n",
      "At epoch  250  cost is:  [[0.35753896]]\n",
      "At epoch  251  cost is:  [[0.35753896]]\n",
      "At epoch  252  cost is:  [[0.35753896]]\n",
      "At epoch  253  cost is:  [[0.35753896]]\n",
      "At epoch  254  cost is:  [[0.35753896]]\n",
      "At epoch  255  cost is:  [[0.35753896]]\n",
      "At epoch  256  cost is:  [[0.35753896]]\n",
      "At epoch  257  cost is:  [[0.35753896]]\n",
      "At epoch  258  cost is:  [[0.35753896]]\n",
      "At epoch  259  cost is:  [[0.35753896]]\n",
      "At epoch  260  cost is:  [[0.35753896]]\n",
      "At epoch  261  cost is:  [[0.35753896]]\n",
      "At epoch  262  cost is:  [[0.35753896]]\n",
      "At epoch  263  cost is:  [[0.35753896]]\n",
      "At epoch  264  cost is:  [[0.35753896]]\n",
      "At epoch  265  cost is:  [[0.35753896]]\n",
      "At epoch  266  cost is:  [[0.35753896]]\n",
      "At epoch  267  cost is:  [[0.35753896]]\n",
      "At epoch  268  cost is:  [[0.35753896]]\n",
      "At epoch  269  cost is:  [[0.35753896]]\n",
      "At epoch  270  cost is:  [[0.35753896]]\n",
      "At epoch  271  cost is:  [[0.35753896]]\n",
      "At epoch  272  cost is:  [[0.35753896]]\n",
      "At epoch  273  cost is:  [[0.35753896]]\n",
      "At epoch  274  cost is:  [[0.35753896]]\n",
      "At epoch  275  cost is:  [[0.35753896]]\n",
      "At epoch  276  cost is:  [[0.35753896]]\n",
      "At epoch  277  cost is:  [[0.35753896]]\n",
      "At epoch  278  cost is:  [[0.35753896]]\n",
      "At epoch  279  cost is:  [[0.35753896]]\n",
      "At epoch  280  cost is:  [[0.35753896]]\n",
      "At epoch  281  cost is:  [[0.35753896]]\n",
      "At epoch  282  cost is:  [[0.35753896]]\n",
      "At epoch  283  cost is:  [[0.35753896]]\n",
      "At epoch  284  cost is:  [[0.35753896]]\n",
      "At epoch  285  cost is:  [[0.35753896]]\n",
      "At epoch  286  cost is:  [[0.35753896]]\n",
      "At epoch  287  cost is:  [[0.35753896]]\n",
      "At epoch  288  cost is:  [[0.35753896]]\n",
      "At epoch  289  cost is:  [[0.35753896]]\n",
      "At epoch  290  cost is:  [[0.35753896]]\n",
      "At epoch  291  cost is:  [[0.35753896]]\n",
      "At epoch  292  cost is:  [[0.35753896]]\n",
      "At epoch  293  cost is:  [[0.35753896]]\n",
      "At epoch  294  cost is:  [[0.35753896]]\n",
      "At epoch  295  cost is:  [[0.35753896]]\n",
      "At epoch  296  cost is:  [[0.35753896]]\n",
      "At epoch  297  cost is:  [[0.35753896]]\n",
      "At epoch  298  cost is:  [[0.35753896]]\n",
      "At epoch  299  cost is:  [[0.35753896]]\n",
      "At epoch  300  cost is:  [[0.35753896]]\n",
      "At epoch  301  cost is:  [[0.35753896]]\n",
      "At epoch  302  cost is:  [[0.35753896]]\n",
      "At epoch  303  cost is:  [[0.35753896]]\n",
      "At epoch  304  cost is:  [[0.35753896]]\n",
      "At epoch  305  cost is:  [[0.35753896]]\n",
      "At epoch  306  cost is:  [[0.35753896]]\n",
      "At epoch  307  cost is:  [[0.35753896]]\n",
      "At epoch  308  cost is:  [[0.35753896]]\n",
      "At epoch  309  cost is:  [[0.35753896]]\n",
      "At epoch  310  cost is:  [[0.35753896]]\n",
      "At epoch  311  cost is:  [[0.35753896]]\n",
      "At epoch  312  cost is:  [[0.35753896]]\n",
      "At epoch  313  cost is:  [[0.35753896]]\n",
      "At epoch  314  cost is:  [[0.35753896]]\n",
      "At epoch  315  cost is:  [[0.35753896]]\n",
      "At epoch  316  cost is:  [[0.35753896]]\n",
      "At epoch  317  cost is:  [[0.35753896]]\n",
      "At epoch  318  cost is:  [[0.35753896]]\n",
      "At epoch  319  cost is:  [[0.35753896]]\n",
      "At epoch  320  cost is:  [[0.35753896]]\n",
      "At epoch  321  cost is:  [[0.35753896]]\n",
      "At epoch  322  cost is:  [[0.35753896]]\n",
      "At epoch  323  cost is:  [[0.35753896]]\n",
      "At epoch  324  cost is:  [[0.35753896]]\n",
      "At epoch  325  cost is:  [[0.35753896]]\n",
      "At epoch  326  cost is:  [[0.35753896]]\n",
      "At epoch  327  cost is:  [[0.35753896]]\n",
      "At epoch  328  cost is:  [[0.35753896]]\n",
      "At epoch  329  cost is:  [[0.35753896]]\n",
      "At epoch  330  cost is:  [[0.35753896]]\n",
      "At epoch  331  cost is:  [[0.35753896]]\n",
      "At epoch  332  cost is:  [[0.35753896]]\n",
      "At epoch  333  cost is:  [[0.35753896]]\n",
      "At epoch  334  cost is:  [[0.35753896]]\n",
      "At epoch  335  cost is:  [[0.35753896]]\n",
      "At epoch  336  cost is:  [[0.35753896]]\n",
      "At epoch  337  cost is:  [[0.35753896]]\n",
      "At epoch  338  cost is:  [[0.35753896]]\n",
      "At epoch  339  cost is:  [[0.35753896]]\n",
      "At epoch  340  cost is:  [[0.35753896]]\n",
      "At epoch  341  cost is:  [[0.35753896]]\n",
      "At epoch  342  cost is:  [[0.35753896]]\n",
      "At epoch  343  cost is:  [[0.35753896]]\n",
      "At epoch  344  cost is:  [[0.35753896]]\n",
      "At epoch  345  cost is:  [[0.35753896]]\n",
      "At epoch  346  cost is:  [[0.35753896]]\n",
      "At epoch  347  cost is:  [[0.35753896]]\n",
      "At epoch  348  cost is:  [[0.35753896]]\n",
      "At epoch  349  cost is:  [[0.35753896]]\n",
      "At epoch  350  cost is:  [[0.35753896]]\n",
      "At epoch  351  cost is:  [[0.35753896]]\n",
      "At epoch  352  cost is:  [[0.35753896]]\n",
      "At epoch  353  cost is:  [[0.35753896]]\n",
      "At epoch  354  cost is:  [[0.35753896]]\n",
      "At epoch  355  cost is:  [[0.35753896]]\n",
      "At epoch  356  cost is:  [[0.35753896]]\n",
      "At epoch  357  cost is:  [[0.35753896]]\n",
      "At epoch  358  cost is:  [[0.35753896]]\n",
      "At epoch  359  cost is:  [[0.35753896]]\n",
      "At epoch  360  cost is:  [[0.35753896]]\n",
      "At epoch  361  cost is:  [[0.35753896]]\n",
      "At epoch  362  cost is:  [[0.35753896]]\n",
      "At epoch  363  cost is:  [[0.35753896]]\n",
      "At epoch  364  cost is:  [[0.35753896]]\n",
      "At epoch  365  cost is:  [[0.35753896]]\n",
      "At epoch  366  cost is:  [[0.35753896]]\n",
      "At epoch  367  cost is:  [[0.35753896]]\n",
      "At epoch  368  cost is:  [[0.35753896]]\n",
      "At epoch  369  cost is:  [[0.35753896]]\n",
      "At epoch  370  cost is:  [[0.35753896]]\n",
      "At epoch  371  cost is:  [[0.35753896]]\n",
      "At epoch  372  cost is:  [[0.35753896]]\n",
      "At epoch  373  cost is:  [[0.35753896]]\n",
      "At epoch  374  cost is:  [[0.35753896]]\n",
      "At epoch  375  cost is:  [[0.35753896]]\n",
      "At epoch  376  cost is:  [[0.35753896]]\n",
      "At epoch  377  cost is:  [[0.35753896]]\n",
      "At epoch  378  cost is:  [[0.35753896]]\n",
      "At epoch  379  cost is:  [[0.35753896]]\n",
      "At epoch  380  cost is:  [[0.35753896]]\n",
      "At epoch  381  cost is:  [[0.35753896]]\n",
      "At epoch  382  cost is:  [[0.35753896]]\n",
      "At epoch  383  cost is:  [[0.35753896]]\n",
      "At epoch  384  cost is:  [[0.35753896]]\n",
      "At epoch  385  cost is:  [[0.35753896]]\n",
      "At epoch  386  cost is:  [[0.35753896]]\n",
      "At epoch  387  cost is:  [[0.35753896]]\n",
      "At epoch  388  cost is:  [[0.35753896]]\n",
      "At epoch  389  cost is:  [[0.35753896]]\n",
      "At epoch  390  cost is:  [[0.35753896]]\n",
      "At epoch  391  cost is:  [[0.35753896]]\n",
      "At epoch  392  cost is:  [[0.35753896]]\n",
      "At epoch  393  cost is:  [[0.35753896]]\n",
      "At epoch  394  cost is:  [[0.35753896]]\n",
      "At epoch  395  cost is:  [[0.35753896]]\n",
      "At epoch  396  cost is:  [[0.35753896]]\n",
      "At epoch  397  cost is:  [[0.35753896]]\n",
      "At epoch  398  cost is:  [[0.35753896]]\n",
      "At epoch  399  cost is:  [[0.35753896]]\n",
      "At epoch  400  cost is:  [[0.35753896]]\n",
      "At epoch  401  cost is:  [[0.35753896]]\n",
      "At epoch  402  cost is:  [[0.35753896]]\n",
      "At epoch  403  cost is:  [[0.35753896]]\n",
      "At epoch  404  cost is:  [[0.35753896]]\n",
      "At epoch  405  cost is:  [[0.35753896]]\n",
      "At epoch  406  cost is:  [[0.35753896]]\n",
      "At epoch  407  cost is:  [[0.35753896]]\n",
      "At epoch  408  cost is:  [[0.35753896]]\n",
      "At epoch  409  cost is:  [[0.35753896]]\n",
      "At epoch  410  cost is:  [[0.35753896]]\n",
      "At epoch  411  cost is:  [[0.35753896]]\n",
      "At epoch  412  cost is:  [[0.35753896]]\n",
      "At epoch  413  cost is:  [[0.35753896]]\n",
      "At epoch  414  cost is:  [[0.35753896]]\n",
      "At epoch  415  cost is:  [[0.35753896]]\n",
      "At epoch  416  cost is:  [[0.35753896]]\n",
      "At epoch  417  cost is:  [[0.35753896]]\n",
      "At epoch  418  cost is:  [[0.35753896]]\n",
      "At epoch  419  cost is:  [[0.35753896]]\n",
      "At epoch  420  cost is:  [[0.35753896]]\n",
      "At epoch  421  cost is:  [[0.35753896]]\n",
      "At epoch  422  cost is:  [[0.35753896]]\n",
      "At epoch  423  cost is:  [[0.35753896]]\n",
      "At epoch  424  cost is:  [[0.35753896]]\n",
      "At epoch  425  cost is:  [[0.35753896]]\n",
      "At epoch  426  cost is:  [[0.35753896]]\n",
      "At epoch  427  cost is:  [[0.35753896]]\n",
      "At epoch  428  cost is:  [[0.35753896]]\n",
      "At epoch  429  cost is:  [[0.35753896]]\n",
      "At epoch  430  cost is:  [[0.35753896]]\n",
      "At epoch  431  cost is:  [[0.35753896]]\n",
      "At epoch  432  cost is:  [[0.35753896]]\n",
      "At epoch  433  cost is:  [[0.35753896]]\n",
      "At epoch  434  cost is:  [[0.35753896]]\n",
      "At epoch  435  cost is:  [[0.35753896]]\n",
      "At epoch  436  cost is:  [[0.35753896]]\n",
      "At epoch  437  cost is:  [[0.35753896]]\n",
      "At epoch  438  cost is:  [[0.35753896]]\n",
      "At epoch  439  cost is:  [[0.35753896]]\n",
      "At epoch  440  cost is:  [[0.35753896]]\n",
      "At epoch  441  cost is:  [[0.35753896]]\n",
      "At epoch  442  cost is:  [[0.35753896]]\n",
      "At epoch  443  cost is:  [[0.35753896]]\n",
      "At epoch  444  cost is:  [[0.35753896]]\n",
      "At epoch  445  cost is:  [[0.35753896]]\n",
      "At epoch  446  cost is:  [[0.35753896]]\n",
      "At epoch  447  cost is:  [[0.35753896]]\n",
      "At epoch  448  cost is:  [[0.35753896]]\n",
      "At epoch  449  cost is:  [[0.35753896]]\n",
      "At epoch  450  cost is:  [[0.35753896]]\n",
      "At epoch  451  cost is:  [[0.35753896]]\n",
      "At epoch  452  cost is:  [[0.35753896]]\n",
      "At epoch  453  cost is:  [[0.35753896]]\n",
      "At epoch  454  cost is:  [[0.35753896]]\n",
      "At epoch  455  cost is:  [[0.35753896]]\n",
      "At epoch  456  cost is:  [[0.35753896]]\n",
      "At epoch  457  cost is:  [[0.35753896]]\n",
      "At epoch  458  cost is:  [[0.35753896]]\n",
      "At epoch  459  cost is:  [[0.35753896]]\n",
      "At epoch  460  cost is:  [[0.35753896]]\n",
      "At epoch  461  cost is:  [[0.35753896]]\n",
      "At epoch  462  cost is:  [[0.35753896]]\n",
      "At epoch  463  cost is:  [[0.35753896]]\n",
      "At epoch  464  cost is:  [[0.35753896]]\n",
      "At epoch  465  cost is:  [[0.35753896]]\n",
      "At epoch  466  cost is:  [[0.35753896]]\n",
      "At epoch  467  cost is:  [[0.35753896]]\n",
      "At epoch  468  cost is:  [[0.35753896]]\n",
      "At epoch  469  cost is:  [[0.35753896]]\n",
      "At epoch  470  cost is:  [[0.35753896]]\n",
      "At epoch  471  cost is:  [[0.35753896]]\n",
      "At epoch  472  cost is:  [[0.35753896]]\n",
      "At epoch  473  cost is:  [[0.35753896]]\n",
      "At epoch  474  cost is:  [[0.35753896]]\n",
      "At epoch  475  cost is:  [[0.35753896]]\n",
      "At epoch  476  cost is:  [[0.35753896]]\n",
      "At epoch  477  cost is:  [[0.35753896]]\n",
      "At epoch  478  cost is:  [[0.35753896]]\n",
      "At epoch  479  cost is:  [[0.35753896]]\n",
      "At epoch  480  cost is:  [[0.35753896]]\n",
      "At epoch  481  cost is:  [[0.35753896]]\n",
      "At epoch  482  cost is:  [[0.35753896]]\n",
      "At epoch  483  cost is:  [[0.35753896]]\n",
      "At epoch  484  cost is:  [[0.35753896]]\n",
      "At epoch  485  cost is:  [[0.35753896]]\n",
      "At epoch  486  cost is:  [[0.35753896]]\n",
      "At epoch  487  cost is:  [[0.35753896]]\n",
      "At epoch  488  cost is:  [[0.35753896]]\n",
      "At epoch  489  cost is:  [[0.35753896]]\n",
      "At epoch  490  cost is:  [[0.35753896]]\n",
      "At epoch  491  cost is:  [[0.35753896]]\n",
      "At epoch  492  cost is:  [[0.35753896]]\n",
      "At epoch  493  cost is:  [[0.35753896]]\n",
      "At epoch  494  cost is:  [[0.35753896]]\n",
      "At epoch  495  cost is:  [[0.35753896]]\n",
      "At epoch  496  cost is:  [[0.35753896]]\n",
      "At epoch  497  cost is:  [[0.35753896]]\n",
      "At epoch  498  cost is:  [[0.35753896]]\n",
      "At epoch  499  cost is:  [[0.35753896]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0cc0464880>]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialising the vector forms\n",
    "X, Y = X_train, Y_train\n",
    "X_t = np.transpose(X)\n",
    "\n",
    "# Initialising the parameters\n",
    "theta = np.zeros((X_train.shape[1], 1), dtype = np.float64)\n",
    "\n",
    "# Initialising the hyperparameters\n",
    "alpha = 0.005\n",
    "epochs = 500\n",
    "\n",
    "# Storing the cost at each epoch\n",
    "cost_list = []\n",
    "\n",
    "m = X.shape[0]\n",
    "n = X.shape[1]\n",
    "\n",
    "# Starting the Gradient Descnet algorithm\n",
    "for i in range(0, epochs):\n",
    "    # Calculating hypothesis\n",
    "    g = h(X, theta)\n",
    "    pd = np.subtract(g, Y)\n",
    "\n",
    "    # Calculating Partial derivative\n",
    "    pd = (X_t @ pd)\n",
    "\n",
    "    # Updating theta\n",
    "    theta = theta - alpha*(pd)\n",
    "\n",
    "    # Finding the cost\n",
    "    cost = J(X, Y, theta)\n",
    "    print(\"At epoch \", i, \" cost is: \", cost)\n",
    "    cost_list.append(cost[0][0])\n",
    "\n",
    "# Plotting cost vs epoch\n",
    "plt.plot(list(range(0, epochs)), cost_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(X_test, theta):\n",
    "    predictions = h(X_test, theta)\n",
    "\n",
    "    # Places 1 where condition is satisfied and 0 otherwise\n",
    "    predictions = np.where(predictions >= 0.5, 1, 0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is:  0.8688524590163934\n"
     ]
    }
   ],
   "source": [
    "# Getting the predictions on test data\n",
    "predictions = get_predictions(X_test, theta)\n",
    "\n",
    "# Getting the accuracy\n",
    "accuracy = (predictions == Y_test).mean()\n",
    "print(\"Accuracy of the model is: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### So, the accuracy of our model is 86.88524%"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
